{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldrouvot/Risk-Detection-in-Construction-Industry/blob/main/Risk_detection__construction_industry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsHfg41rcyYv"
      },
      "source": [
        "# Projet de d√©tection des accidents de travail dans un contexte BTP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaF2Hjpgc-cM"
      },
      "source": [
        "# üíªSetup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fkIRmSe3Al3t",
        "outputId": "5526b744-7a06-49f5-be44-f86dc9282816",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr8kvJGscdGE",
        "outputId": "5cfc8e67-0a7f-4114-eded-23899a54fa34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Stage et projet/projet/projet deep learning/Notebook\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Stage et projet/projet/projet deep learning/Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX2gXGwRdB-E",
        "outputId": "e02e9351-27ec-4be8-d441-8f18f862cb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'missingno'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8a7408adb070>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'missingno'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import time\n",
        "from datetime import datetime\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZZM5DD9f3q6"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zc-IedComAm8"
      },
      "outputs": [],
      "source": [
        "# excel_file_1 = (\"/content/drive/MyDrive/Stage et projet/projet/projet deep learning/DonneÃÅes/Datas_20231207.xlsx\")\n",
        "excel_file_1 = (\"/content/drive/MyDrive/projet deep learning/DonneÃÅes/Datas_20231207.xlsx\")\n",
        "xls_1 = pd.ExcelFile(excel_file_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWobd-HkdeiT"
      },
      "outputs": [],
      "source": [
        "# first excel\n",
        "sheet_names = xls_1.sheet_names\n",
        "dict_df = {}\n",
        "for sheet_name in sheet_names:\n",
        "    dict_df[sheet_name] = xls_1.parse(sheet_name)\n",
        "for sheet_name, df in dict_df.items():\n",
        "    print(f\"Nom de la feuille : {sheet_name}\")\n",
        "    # print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVrdFfCqhChc"
      },
      "outputs": [],
      "source": [
        "dict_df.keys()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nUpilFUUanY"
      },
      "source": [
        "#üíπFonctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMXhWq6ZoSEb"
      },
      "outputs": [],
      "source": [
        "def missing_percentage(df, figsize=(24, 6)):\n",
        "  \"\"\"A function for plotting missing ratios.\"\"\"\n",
        "\n",
        "  total = df.isnull().sum().sort_values(\n",
        "      ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n",
        "  percent = (df.isnull().sum().sort_values(ascending=False) / len(df) *\n",
        "              100)[(df.isnull().sum().sort_values(ascending=False) / len(df) *\n",
        "                    100) != 0]\n",
        "  missing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  sns.barplot(x=missing.index, y='Percent', data=missing, palette='coolwarm_r')\n",
        "  plt.xticks(rotation=90)\n",
        "\n",
        "  display(missing.T.style.background_gradient(cmap='Reds', axis=1))\n",
        "\n",
        "def calcul_annee(x):\n",
        "   return ((datetime.today()-x).days)/365\n",
        "\n",
        "\n",
        "def plot_scatter_with_regression(df, x_feature, y_feature, xlim=200, ylim=100000, figsize=(15, 15)):\n",
        "    \"\"\"\n",
        "    Plot a scatter plot with regression line for two features from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing the features.\n",
        "    - x_feature (str): The feature for the x-axis.\n",
        "    - y_feature (str): The feature for the y-axis.\n",
        "    - xlim (int): The upper limit for x-axis values to be displayed. Default is 200.\n",
        "    - ylim (int): The upper limit for y-axis values to be displayed. Default is 100000.\n",
        "    - figsize (tuple): Size of the resulting plot.\n",
        "\n",
        "    Returns:\n",
        "    - None: Displays a scatter plot with regression line.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set font scale for better readability\n",
        "    sns.set(font_scale=1.1)\n",
        "\n",
        "    # Filter the dataframe based on provided limits for x and y features\n",
        "    filtered_df = df[(df[x_feature] < xlim) & (df[y_feature] < ylim)]\n",
        "\n",
        "    # Create the scatter plot with regression line\n",
        "    sns.lmplot(data=filtered_df, x=x_feature, y=y_feature, line_kws={'color': 'red'})\n",
        "\n",
        "    # Set plot labels and title\n",
        "    plt.xlabel(x_feature)\n",
        "    plt.ylabel(y_feature)\n",
        "    plt.title(f\"Scatter Plot of {x_feature} and {y_feature} with Regression Line\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "def plot_correlation_with_feature_v2(df, target_feature, threshold=.3, figsize=(10, 8), cmap='coolwarm'):\n",
        "    \"\"\"\n",
        "    Plot the correlation of a target feature with other numeric features in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The input DataFrame containing numeric features.\n",
        "    - target_feature (str): The feature for which correlations with other features are to be plotted.\n",
        "    - figsize (tuple): Size of the resulting plot.\n",
        "\n",
        "    Returns:\n",
        "    - None: Displays a horizontal bar plot of correlations.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the correlation between the target feature and other numeric features in the DataFrame\n",
        "    feature_corr = df.corr()[target_feature]\n",
        "\n",
        "    # Drop the correlation of the target feature with itself to avoid redundancy\n",
        "    feature_corr = feature_corr.drop(target_feature)\n",
        "\n",
        "    # Take the absolute values of the correlation ratios for better comparison\n",
        "    feature_corr_abs = feature_corr.abs()\n",
        "\n",
        "    # Sort the correlation values in descending order based on their absolute values\n",
        "    feature_corr_sorted = feature_corr_abs.sort_values(ascending=False)\n",
        "\n",
        "    # Define custom color palettes (diverging and sequential palettes are defined but only the diverging one is used in this case)\n",
        "    cmap = sns.color_palette('coolwarm_r', n_colors=len(feature_corr_sorted))\n",
        "\n",
        "    # Set font scale for better readability\n",
        "    sns.set(font_scale=1.1)\n",
        "\n",
        "    # Create a new figure with the provided figsize\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Create a horizontal bar plot displaying the correlation of the target feature with other features\n",
        "    sns.barplot(x=feature_corr_sorted.values, y=feature_corr_sorted.index, palette=cmap)\n",
        "\n",
        "    # Set plot labels and title\n",
        "    plt.xlabel(f\"Absolute Correlation with {target_feature}\")\n",
        "    plt.ylabel(\"Features\")\n",
        "    plt.title(f\"Correlation of {target_feature} with other Features (Sorted by Absolute Correlation)\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiI1rArukGDd"
      },
      "source": [
        "#Cr√©ation des dataframes et de leurs copies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ2_aVtpkJ6e"
      },
      "outputs": [],
      "source": [
        "df_acc = dict_df['Acc Trav']\n",
        "df_acc_c = pd.DataFrame(df_acc)\n",
        "df_donnee = dict_df['Donn√©es personnelles']\n",
        "df_donnee_c = pd.DataFrame(df_donnee)\n",
        "df_meteo = dict_df['M√©t√©o']\n",
        "df_meteo_c = pd.DataFrame(df_meteo)\n",
        "df_abs = dict_df['Absences']\n",
        "df_abs_c = pd.DataFrame(df_abs)\n",
        "df_me = dict_df['Messages envoy√©s']\n",
        "df_me_c = pd.DataFrame(df_me)\n",
        "df_mr = dict_df['Messages re√ßus']\n",
        "df_mr_c = pd.DataFrame(df_mr)\n",
        "df_evenements = dict_df['Evenements']\n",
        "df_evenements_c = pd.DataFrame(df_evenements)\n",
        "df_dmd = dict_df['Demandes']\n",
        "df_dmd_c = pd.DataFrame(df_dmd)\n",
        "df_hab = dict_df['Habilitations']\n",
        "df_hab_c = pd.DataFrame(df_hab)\n",
        "df_formations = dict_df['Formations']\n",
        "df_formations_c = pd.DataFrame(df_formations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQDcbwVjhmnY"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZpsOD6miOLE"
      },
      "outputs": [],
      "source": [
        "#encoding de l'accident de travail\n",
        "df_acc_c['AT reconnu?'] = df_acc_c['AT reconnu?'].replace(\"oui\", 1)\n",
        "df_acc_c['AT reconnu?']= df_acc_c['AT reconnu?'].replace(\"non\", 0)\n",
        "\n",
        "#encoding messages envoy√©s\n",
        "df_me_c['Type pr√©vention ?'] = df_me_c['Type pr√©vention ?'].replace(\"oui\", 1)\n",
        "df_me_c['Type pr√©vention ?']= df_me_c['Type pr√©vention ?'].replace(\"non\", 0)\n",
        "\n",
        "df_me_c['Positif ?'] = df_me_c['Positif ?'].replace(\"oui\", 1)\n",
        "df_me_c['Positif ?']= df_me_c['Positif ?'].replace(\"non\", 0)\n",
        "\n",
        "#encoding messages recus\n",
        "df_mr_c['Type pr√©vention ?'] = df_mr_c['Type pr√©vention ?'].replace(\"oui\", 1)\n",
        "df_mr_c['Type pr√©vention ?']= df_mr_c['Type pr√©vention ?'].replace(\"non\", 0)\n",
        "\n",
        "df_mr_c['Positif ?'] = df_mr_c['Positif ?'].replace(\"oui\", 1)\n",
        "df_mr_c['Positif ?']= df_mr_c['Positif ?'].replace(\"non\", 0)\n",
        "\n",
        "#encoding donn√©es personnelles\n",
        "df_donnee_c['Civilit√©'] = df_donnee_c['Civilit√©'].replace(\"Monsieur\", 0)\n",
        "df_donnee_c['Civilit√©'] = df_donnee_c['Civilit√©'].replace(\"Madame\", 1)\n",
        "df_donnee_c['Type de contrat'].replace(\"Contrat d'apprentissage entreprises non artisanales de plus de 10 salari√©s (loi de 1987)\", 'CAP', inplace=True,)\n",
        "df_donnee_c['Type de contrat'].replace(\"Contrat d'apprentissage entreprises artisanales ou de 10 salari√©s au plus (loi de 1979)\", 'CAP', inplace=True,)\n",
        "df_donnee_c['Type de contrat'].replace(\"Contrat de professionnalisation\", 'CP', inplace=True,)\n",
        "df_donnee_c['Type de contrat'] = df_donnee_c['Type de contrat'].replace(\"Stage\",0)\n",
        "df_donnee_c['Type de contrat'] = df_donnee_c['Type de contrat'].replace(\"CAP\",1)\n",
        "df_donnee_c['Type de contrat'] = df_donnee_c['Type de contrat'].replace(\"CP\",1)\n",
        "df_donnee_c['Type de contrat'] = df_donnee_c['Type de contrat'].replace(\"CDD\",2)\n",
        "df_donnee_c['Type de contrat'] = df_donnee_c['Type de contrat'].replace(\"CDI\",3)\n",
        "df_donnee_c['V√©hicule'] = df_donnee_c['V√©hicule'].replace(\"avec vehicule\", 1)\n",
        "df_donnee_c['V√©hicule'].fillna(0,inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#encoding meteo\n",
        "df_meteo_c['Humeur'] = df_meteo_c['Humeur'].replace(\"Pluie\", -1)\n",
        "df_meteo_c['Humeur'] = df_meteo_c['Humeur'].replace(\"Nuage\", 0)\n",
        "df_meteo_c['Humeur'] = df_meteo_c['Humeur'].replace(\"Soleil\", 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_donnee_c"
      ],
      "metadata": {
        "id": "gsjZp3M4KYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNz7mfcJCu5j"
      },
      "source": [
        "#üü•Analyse univariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdJTfIwDodU4"
      },
      "source": [
        "##üü¶Analyse Accidents travails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcdT3Mi2iDEZ"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHPJVv-JnNrM"
      },
      "outputs": [],
      "source": [
        "df_acc_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooylknbYn4QH"
      },
      "outputs": [],
      "source": [
        "df_acc_c.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9As55h2GpYo8"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_acc_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "#missing_percentage(df_acc_c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSUbGVHfpjRp"
      },
      "outputs": [],
      "source": [
        "df_acc_c['AT reconnu?'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9Szd55NszoP"
      },
      "outputs": [],
      "source": [
        "df_acc_c['Localisation'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOEV5pehp5iU"
      },
      "outputs": [],
      "source": [
        "### Tr√©s peu de \"non\" --> suppression OU remplissage avec classe majoritaire = oui\"\n",
        "df_acc_c = df_acc_c.dropna(inplace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WSP6EZrsbqB"
      },
      "outputs": [],
      "source": [
        "print(df_acc_c.isna().any())\n",
        "df_acc_c.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRx18XyO0SsT"
      },
      "source": [
        "## üü¶Analyse Donn√©es personnelles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5uw6nanVGtD"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYjmf-wR0U8C"
      },
      "outputs": [],
      "source": [
        "df_donnee_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-kAnES9UpJb"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_donnee_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "missing_percentage(df_donnee_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTZSAmhGV3kS"
      },
      "outputs": [],
      "source": [
        "df_donnee_c['Type de contrat'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1Ua79WBMEBr"
      },
      "outputs": [],
      "source": [
        "df_donnee_c['Anciennet√©']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BULWdu2kee_R"
      },
      "source": [
        "On voit qu'il manque beaucoup de donn√©es pour l'anciennet√© mais que pour quasiment toutes les autres la date d'anciennet√© est identique √† celle du d√©but de contrat, nous allons donc remplacer l'anciennet√© par une dur√©e en ann√©e cr√©e √† partir de la date de contrat pour un chaque employ√©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdC7lKILzpHl"
      },
      "source": [
        "Travail sur Anciennet√©:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlCzIm3eW6Gk"
      },
      "outputs": [],
      "source": [
        "#Changement des valeurs nuls de Anciennet√© par celles de D√©but 1er contrat + verififcation\n",
        "\n",
        "\n",
        "df_donnee_c['Anciennet√©'].fillna(df_donnee_c['D√©but 1e contrat'],inplace = True)\n",
        "\n",
        "assert not(df_donnee_c['Anciennet√©'].isna().any())\n",
        "\n",
        "df_donnee_c[['Anciennet√©','D√©but 1e contrat']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_HAv4fsa2TJ"
      },
      "outputs": [],
      "source": [
        "# Remplacer la date d'anciennet√© par une valeur en ann√©e\n",
        "\n",
        "df_donnee_c['Anciennet√©']=df_donnee_c['Anciennet√©'].apply(calcul_annee)\n",
        "df_donnee_c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvyOjLE_j0sp"
      },
      "source": [
        "On peut maintenant supprimer la colonne d√©but 1er contrat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYpYvFnazvbe"
      },
      "source": [
        "Travail sur date de naissance :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFXfQaNjj6wp"
      },
      "outputs": [],
      "source": [
        "# a faire ou pas ? simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW4VJ3EpkfcX"
      },
      "outputs": [],
      "source": [
        "# Remplacer la date de naissance par l'age\n",
        "\n",
        "df_donnee_c['Date de naissance']=df_donnee_c['Date de naissance'].apply(calcul_annee)\n",
        "df_donnee_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUj-7MCKlIZ3"
      },
      "outputs": [],
      "source": [
        "# on change le nom de la colonne par Age\n",
        "\n",
        "df_donnee_c.rename(columns={\"Date de naissance\": \"Age\"}, inplace=True)\n",
        "df_donnee_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ExcfvJK0Wxq"
      },
      "source": [
        "Travail sur Civilit√©:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ7syUIF0aEy"
      },
      "outputs": [],
      "source": [
        "# on change le nom de la colonne par Sexe\n",
        "\n",
        "df_donnee_c.rename(columns={\"Civilit√©\": \"Sexe\"}, inplace=True)\n",
        "df_donnee_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohl7C2X_notl"
      },
      "source": [
        "## üü¶Analyse M√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDL-fgQOocPi"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4dKd45HoliW"
      },
      "outputs": [],
      "source": [
        "df_meteo_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMxfcbvFysz8"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_meteo_c.isna().any())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UxPC0xhnslV"
      },
      "source": [
        "## üü¶Analyse Absences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLz0J8WodA6"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVcb4B-couks"
      },
      "outputs": [],
      "source": [
        "print(df_abs_c['Type absence'].unique())\n",
        "\n",
        "df_abs_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPq-Dv3dzF8H"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_abs_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "#missing_percentage(df_abs_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laMmRr0N7OAm"
      },
      "outputs": [],
      "source": [
        "#Cr√©ation d'une colonne dur√©e absence √† partir de d√©but absence et fin absence\n",
        "\n",
        "df_abs_c['duree_absence'] = (df_abs_c['Fin absence'] - df_abs_c['D√©but absence']) / np.timedelta64(1, 'D')\n",
        "df_abs_c = df_abs_c[['Identifiant salari√©','D√©but absence', 'Fin absence', 'duree_absence', 'Type absence']]\n",
        "\n",
        "df_abs_c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQ48VWInswp"
      },
      "source": [
        "## üü¶Analyse Messages envoy√©s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SykqGl_codnL"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpPLSZRfoys9"
      },
      "outputs": [],
      "source": [
        "df_me_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmr0aR_izP2I"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_me_c.isna().any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D1mGyPBns2B"
      },
      "source": [
        "## üü¶Analyse Messages re√ßus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LPXww35oeNh"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-_BW8mmpG9L"
      },
      "outputs": [],
      "source": [
        "df_mr_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pid-DyjCzWWu"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_mr_c.isna().any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzoaQDUqns6K"
      },
      "source": [
        "## üü¶Analyse Evenements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yABEsLboezk"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThFQ7YW2pLL9"
      },
      "outputs": [],
      "source": [
        "df_evenements_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8DNL_DvzcBz"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_evenements_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "missing_percentage(df_evenements_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WACcsimNnyKZ"
      },
      "source": [
        "## üü¶Analyse Demandes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4csNTCzRofmc"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9HKf3yipTVu"
      },
      "outputs": [],
      "source": [
        "df_dmd_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUYRXttFzmnQ"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_dmd_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "#missing_percentage(df_dmd_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwCW3yfRns9U"
      },
      "source": [
        "## üü¶Analyse Habilitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veAoGuPmogOX"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewUIvxVQpXpP"
      },
      "outputs": [],
      "source": [
        "df_hab_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BittlIAKzszJ"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_hab_c.isna().any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooy55atZnySc"
      },
      "source": [
        "## üü¶Analyse Formations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYp6noRohCh"
      },
      "source": [
        "### Affichage et travail sur les donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmZ8_8QCpa2A"
      },
      "outputs": [],
      "source": [
        "df_formations_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdFWLlzJzxpX"
      },
      "outputs": [],
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_formations_c.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "missing_percentage(df_formations_c)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üéÅ Colonne numerique / categorique / unique"
      ],
      "metadata": {
        "id": "m05hbYPzUDvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col = ['Sexe','Type de contrat','AT reconnu?', 'Humeur']\n",
        "numerical_col = ['Frequence demande', 'Frequence accident', 'Frequence message recu', 'Frequence message envoy√©', 'Frequence absence', 'Age', 'Anciennet√©', ]\n",
        "unique_col = ['Identifiant salari√©']"
      ],
      "metadata": {
        "id": "V1wt0hWwUI_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üü•Analyse multivariate"
      ],
      "metadata": {
        "id": "wKSR1EM1BKSA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62dZePDy2in1"
      },
      "source": [
        "On va reamenager nos dataframe en gardant que les colonnes que l'on souhaite et qui nous semble importante pour ensuite les li√©s."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üü¶Cr√©ation d'un dataframe √† partir de tous les autres"
      ],
      "metadata": {
        "id": "NHWutjSf00XQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataframe avec identifiant salari√© unique"
      ],
      "metadata": {
        "id": "sIKUq6zOLt4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Choix des features importantes pour chaque **dataframe**"
      ],
      "metadata": {
        "id": "5hZAcV1G1Pxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcHNdYRm2rGG"
      },
      "outputs": [],
      "source": [
        "df_cor_c = df_acc_c.copy(deep=True)\n",
        "del df_cor_c['Localisation']\n",
        "del df_cor_c['Description']\n",
        "del df_cor_c['AT reconnu?']\n",
        "del df_cor_c['date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jh3Vf_I3UGS"
      },
      "outputs": [],
      "source": [
        "df_cor_donnee = df_donnee_c.copy(deep=True)\n",
        "del df_cor_donnee['D√©but 1e contrat']\n",
        "del df_cor_donnee['Situation familiale']\n",
        "del df_cor_donnee['Emploi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr2s9S244NoS"
      },
      "outputs": [],
      "source": [
        "df_cor_abs = df_abs_c.copy(deep=True)\n",
        "del df_cor_abs['Type absence']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cor_me = df_me_c.copy(deep=True)\n",
        "del df_cor_me['Type de message']\n",
        "del df_cor_me['Texte message']"
      ],
      "metadata": {
        "id": "tmePg0eHn0TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQQA-wwX47tq"
      },
      "outputs": [],
      "source": [
        "df_cor_mr = df_mr_c.copy(deep=True)\n",
        "del df_cor_mr['Type de message']\n",
        "del df_cor_mr['Texte message']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWP4D8YT5DXW"
      },
      "outputs": [],
      "source": [
        "df_cor_evt = df_evenements_c.copy(deep=True)\n",
        "# utilit√© ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhjO-PbK5UNc"
      },
      "outputs": [],
      "source": [
        "df_cor_dmd = df_dmd_c.copy(deep=True)\n",
        "del df_cor_dmd['Demande']\n",
        "del df_cor_dmd['Type de demande']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQHLwyU5qZx"
      },
      "outputs": [],
      "source": [
        "# habilitations et formations on garde en dataframe qualitatif"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####On les regroupe"
      ],
      "metadata": {
        "id": "zXRrjLAC1ZsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoBhXUdVci9A"
      },
      "outputs": [],
      "source": [
        "freq = df_cor_abs['Identifiant salari√©'].value_counts().sort_index().rename_axis('Identifiant salari√©').to_frame('Nombre absence')\n",
        "df_cor = pd.merge(freq,df_cor_donnee,how=\"outer\",on='Identifiant salari√©')\n",
        "df_cor['Nombre absence'] = df_cor['Nombre absence'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq = df_cor_dmd['Identifiant salari√©'].value_counts().sort_index().rename_axis('Identifiant salari√©').to_frame('Nombre demande')\n",
        "df_cor = pd.merge(freq,df_cor,how=\"outer\",on='Identifiant salari√©')\n",
        "df_cor['Nombre demande'] = df_cor['Nombre demande'].fillna(0)"
      ],
      "metadata": {
        "id": "ffD22wBjlf_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = df_cor_me['Identifiant salari√©'].value_counts().sort_index().rename_axis('Identifiant salari√©').to_frame('Nombre message envoy√©')\n",
        "df_cor = pd.merge(freq,df_cor,how=\"outer\",on='Identifiant salari√©')\n",
        "df_cor['Nombre message envoy√©'] = df_cor['Nombre message envoy√©'].fillna(0)"
      ],
      "metadata": {
        "id": "rdKLwUYKnftO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = df_cor_mr['Identifiant salari√©'].value_counts().sort_index().rename_axis('Identifiant salari√©').to_frame('Nombre message recu')\n",
        "df_cor = pd.merge(freq,df_cor,how=\"outer\",on='Identifiant salari√©')\n",
        "df_cor['Nombre message recu'] = df_cor['Nombre message recu'].fillna(0)"
      ],
      "metadata": {
        "id": "RI_fHvUXnfoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq = df_cor_c['Identifiant salari√©'].value_counts().sort_index().rename_axis('Identifiant salari√©').to_frame('Nombre accident')\n",
        "df_cor = pd.merge(freq,df_cor,how=\"outer\",on='Identifiant salari√©')\n",
        "df_cor['Nombre accident'] = df_cor['Nombre accident'].fillna(0)"
      ],
      "metadata": {
        "id": "RQ1clfP9D1HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons un dataframe complet mais il reste quelques modifications"
      ],
      "metadata": {
        "id": "7xUxs2sV1hTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# y a t'il des donn√©es manquantes ?\n",
        "\n",
        "print(df_cor.isna().any())\n",
        "\n",
        "# Oui, on veut les voir\n",
        "\n",
        "#missing_percentage(df_cor)"
      ],
      "metadata": {
        "id": "nt3Y8rww4-vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il faut changer les nombres de demandes, nombres de messages etc par une frequence en fonction de l'anciennet√© de la personne sinon ca n'a pas de sens."
      ],
      "metadata": {
        "id": "cQNjd2kQ6yv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cor['Nombre absence'] = (df_cor['Nombre absence']/df_cor['Anciennet√©'])\n",
        "df_cor.rename(columns={'Nombre absence': 'Frequence absence'},inplace=True)\n",
        "\n",
        "df_cor['Nombre message recu'] = (df_cor['Nombre message recu']/df_cor['Anciennet√©'])\n",
        "df_cor.rename(columns={'Nombre message recu': 'Frequence message recu'},inplace= True)\n",
        "\n",
        "df_cor['Nombre message envoy√©'] = (df_cor['Nombre message envoy√©']/df_cor['Anciennet√©'])\n",
        "df_cor.rename(columns={'Nombre message envoy√©': 'Frequence message envoy√©'},inplace=True)\n",
        "\n",
        "df_cor['Nombre accident'] = (df_cor['Nombre accident']/df_cor['Anciennet√©'])\n",
        "df_cor.rename(columns={'Nombre accident': 'Frequence accident'},inplace=True)\n",
        "\n",
        "df_cor['Nombre demande'] = (df_cor['Nombre demande']/df_cor['Anciennet√©'])\n",
        "df_cor.rename(columns={'Nombre demande': 'Frequence demande'},inplace=True)\n",
        "df_cor\n"
      ],
      "metadata": {
        "id": "hgccBX3q7ALo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataframe pour les times series"
      ],
      "metadata": {
        "id": "XTDQtds7Nvfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Choix des features importantes pour chaque **dataframe**"
      ],
      "metadata": {
        "id": "QG8cJHvgP6j9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfcgbdmPPfbe"
      },
      "outputs": [],
      "source": [
        "df_cor2_dmd = df_dmd_c.copy(deep=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIpGJ6PPPYrn"
      },
      "outputs": [],
      "source": [
        "df_cor2_evt = df_evenements_c.copy(deep=True)\n",
        "# utilit√© ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrynq4IIPTYb"
      },
      "outputs": [],
      "source": [
        "df_cor2_mr = df_mr_c.copy(deep=True)\n",
        "del df_cor2_mr['Type de message']\n",
        "del df_cor2_mr['Texte message']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cor2_me = df_me_c.copy(deep=True)\n",
        "del df_cor2_me['Type de message']\n",
        "del df_cor2_me['Texte message']"
      ],
      "metadata": {
        "id": "CjFphKiVPNQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uBpeMdqPHc6"
      },
      "outputs": [],
      "source": [
        "df_cor2_abs = df_abs_c.copy(deep=True)\n",
        "del df_cor2_abs['Type absence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucUSldZKPA2q"
      },
      "outputs": [],
      "source": [
        "df_cor2_meteo = df_meteo_c.copy(deep=True)\n",
        "\n",
        "df_cor2_meteo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqagJyGSOy37"
      },
      "outputs": [],
      "source": [
        "df_cor2_donnee = df_donnee_c.copy(deep=True)\n",
        "#del df_cor2_donnee['D√©but 1e contrat']\n",
        "del df_cor2_donnee['Situation familiale']\n",
        "del df_cor2_donnee['Emploi']\n",
        "df_cor2_donnee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP-SPQNlOrbr"
      },
      "outputs": [],
      "source": [
        "df_cor2_c = df_acc_c.copy(deep=True)\n",
        "#df_cor2_c = df_cor2_c.set_index('date')\n",
        "df_trie = df_cor2_c.sort_values(by='date', ascending=False)\n",
        "df_trie"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cr√©ation des vecteurs"
      ],
      "metadata": {
        "id": "EH5lz_2h-Y9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©ation d'un tableau vide avec toutes les dates entre 2008 et 2017\n",
        "date_range_2008_2017 = pd.date_range(start='2008-01-01', end='2017-12-31', freq='2W')\n",
        "all_dates_2008_2017 = pd.DataFrame(date_range_2008_2017, columns=['Start_Date'])\n",
        "\n",
        "# Cr√©ation d'un tableau vide avec toutes les dates entre 2018 et 2023\n",
        "date_range_2018_2023 = pd.date_range(start='2018-01-01', end='2023-12-31', freq='2W')\n",
        "all_dates_2018_2023 = pd.DataFrame(date_range_2018_2023, columns=['Start_Date'])\n",
        "\n",
        "# Cr√©er un dictionnaire pour stocker les vecteurs d'accidents pour les ann√©es 2008 √† 2017 et 2018 √† 2023\n",
        "accident_vectors_2008_2017 = {}\n",
        "accident_vectors_2018_2023 = {}\n",
        "\n",
        "# It√©ration sur chaque identifiant de salari√©\n",
        "for id_salarie in df_cor2_donnee['Identifiant salari√©'].unique():\n",
        "    # Filtrer les donn√©es pour un identifiant de salari√© donn√©\n",
        "    df_filtered = df_cor2_c[df_cor2_c['Identifiant salari√©'] == id_salarie]\n",
        "\n",
        "    # Cr√©er un vecteur pour stocker les accidents pour les ann√©es 2008 √† 2017\n",
        "    accident_vector_2008_2017 = np.zeros(len(all_dates_2008_2017), dtype=int)\n",
        "\n",
        "    # It√©ration sur chaque p√©riode de deux semaines pour les ann√©es 2008 √† 2017\n",
        "    for i, start_date in enumerate(all_dates_2008_2017['Start_Date']):\n",
        "        end_date = start_date + pd.DateOffset(weeks=2)\n",
        "        accidents_in_period = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] < end_date)]\n",
        "        accident_vector_2008_2017[i] = len(accidents_in_period)\n",
        "\n",
        "    # Stocker le vecteur d'accidents pour les ann√©es 2008 √† 2017 dans le dictionnaire\n",
        "    accident_vectors_2008_2017[id_salarie] = accident_vector_2008_2017\n",
        "\n",
        "    # Cr√©er un vecteur pour stocker les accidents pour les ann√©es 2018 √† 2023\n",
        "    accident_vector_2018_2023 = np.zeros(len(all_dates_2018_2023), dtype=int)\n",
        "\n",
        "    # It√©ration sur chaque p√©riode de deux semaines pour les ann√©es 2018 √† 2023\n",
        "    for i, start_date in enumerate(all_dates_2018_2023['Start_Date']):\n",
        "        end_date = start_date + pd.DateOffset(weeks=2)\n",
        "        accidents_in_period = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] < end_date)]\n",
        "        accident_vector_2018_2023[i] = len(accidents_in_period)\n",
        "\n",
        "    # Stocker le vecteur d'accidents pour les ann√©es 2018 √† 2023 dans le dictionnaire\n",
        "    accident_vectors_2018_2023[id_salarie] = accident_vector_2018_2023\n",
        "\n",
        "\n",
        "# # Afficher les vecteurs d'accidents pour chaque salari√© pour les ann√©es 2008 √† 2017\n",
        "# print(\"Vecteurs d'accidents pour les ann√©es 2008 √† 2017 :\")\n",
        "# for id_salarie, vector in accident_vectors_2008_2017.items():\n",
        "#     print(f\"Identifiant salari√©: {id_salarie}, Vecteur d'accidents: {vector}\")\n",
        "\n",
        "# Afficher les vecteurs d'accidents pour chaque salari√© pour les ann√©es 2018 √† 2023\n",
        "# print(\"\\nVecteurs d'accidents pour les ann√©es 2018 √† 2023 :\")\n",
        "# for id_salarie, vector in accident_vectors_2018_2023.items():\n",
        "#     print(f\"Identifiant salari√©: {id_salarie}, Vecteur d'accidents: {vector}\")\n"
      ],
      "metadata": {
        "id": "ArdVZP9fdAJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It√©ration sur chaque identifiant de salari√©\n",
        "for id_salarie in df_cor2_c['Identifiant salari√©'].unique():\n",
        "    # Filtrer les donn√©es pour un identifiant de salari√© donn√©\n",
        "    df_filtered = df_cor2_c[df_cor2_c['Identifiant salari√©'] == id_salarie]\n",
        "\n",
        "    # R√©cup√©rer la date de d√©but du premier contrat du salari√©\n",
        "    debut_contrat = df_cor2_donnee[df_cor2_donnee['Identifiant salari√©'] == id_salarie]['D√©but 1e contrat'].iloc[0]\n",
        "    print(f\"Date de d√©but du contrat pour le salari√© {id_salarie} : {debut_contrat}\")\n",
        "\n",
        "    # Convertir les dates en objets datetime pour la comparaison\n",
        "    debut_contrat_datetime = debut_contrat.to_pydatetime()\n",
        "    print(f\"Type de debut_contrat_datetime : {type(debut_contrat_datetime)}\")\n",
        "    print(f\"Type de datetime(2018, 1, 1) : {type(datetime(2018, 1, 1))}\")\n",
        "    print(f\"Type de datetime(2023, 12, 31) : {type(datetime(2023, 12, 31))}\")\n",
        "\n",
        "    # V√©rifier si la date de d√©but de contrat est dans la plage 2018-2023\n",
        "    if datetime(2018, 1, 1) <= debut_contrat_datetime and debut_contrat_datetime <= datetime(2023, 12, 31):\n",
        "        # Trouver l'indice de d√©but de contrat dans les dates 2018-2023\n",
        "        matching_dates = all_dates_2018_2023[all_dates_2018_2023['Start_Date'] == debut_contrat]\n",
        "        print(f\"Traitement du salari√© avec l'identifiant : {id_salarie}\")\n",
        "        if not matching_dates.empty:\n",
        "            start_index = matching_dates.index[0]\n",
        "\n",
        "            # Cr√©er un vecteur pour stocker les accidents pour les ann√©es 2018 √† 2023\n",
        "            accident_vector_2018_2023 = np.zeros(len(all_dates_2018_2023), dtype=int)\n",
        "\n",
        "            # Copier les valeurs du vecteur de la date de d√©but de contrat jusqu'en 2023\n",
        "            accident_vector_2018_2023[:start_index] = accident_vectors_2008_2017[id_salarie][-start_index:]\n",
        "\n",
        "            # It√©ration sur chaque p√©riode de deux semaines pour les ann√©es 2018 √† 2023\n",
        "            for i, start_date in enumerate(all_dates_2018_2023['Start_Date'][start_index:]):\n",
        "                end_date = start_date + pd.DateOffset(weeks=2)\n",
        "                accidents_in_period = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] < end_date)]\n",
        "                accident_vector_2018_2023[i + start_index] = len(accidents_in_period)\n",
        "\n",
        "            # Stocker le vecteur d'accidents pour les ann√©es 2018 √† 2023 dans le dictionnaire\n",
        "            accident_vectors_2018_2023[id_salarie] = accident_vector_2018_2023\n",
        "        else:\n",
        "            print(f\"Ignorer le salari√© {id_salarie} car sa date de d√©but de contrat n'est pas trouv√©e dans les dates 2018-2023.\")\n",
        "    else:\n",
        "        # Si la date de d√©but de contrat n'est pas dans la plage 2018-2023, ignorer ce salari√©\n",
        "        print(f\"Ignorer le salari√© {id_salarie} car sa date de d√©but de contrat n'est pas dans la plage 2018-2023.\")\n"
      ],
      "metadata": {
        "id": "my6c1RQRIj2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debut_contrat= df_cor2_donnee[df_cor2_donnee['Identifiant salari√©'] == 346]['D√©but 1e contrat'].iloc[0]\n",
        "matching_dates = all_dates_2018_2023[all_dates_2018_2023['Start_Date'] == debut_contrat]\n",
        "matching_dates"
      ],
      "metadata": {
        "id": "Ka19XF5DQMWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "humeur_vectors_2008_2017 = {}\n",
        "humeur_vectors_2018_2023 = {}\n",
        "# It√©ration sur chaque identifiant de salari√©\n",
        "for id_salarie in df_cor2_meteo['Identifiant salari√©'].unique():\n",
        "    df_filtered = df_cor2_meteo[df_cor2_meteo['Identifiant salari√©'] == id_salarie]\n",
        "\n",
        "    # Cr√©er un vecteur pour stocker les moyennes de 'humeur' pour les ann√©es 2008 √† 2017\n",
        "    humeur_vector_2008_2017 = np.zeros(len(all_dates_2008_2017), dtype=float)\n",
        "\n",
        "    # It√©ration sur chaque p√©riode de deux semaines pour les ann√©es 2008 √† 2017\n",
        "    for i, start_date in enumerate(all_dates_2008_2017['Start_Date']):\n",
        "        end_date = start_date + pd.DateOffset(weeks=2)\n",
        "        df_period = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] < end_date)]\n",
        "        if not df_period.empty:\n",
        "            humeur_vector_2008_2017[i] = pd.to_numeric(df_period['Humeur'], errors='coerce').mean()\n",
        "\n",
        "    # Stocker le vecteur de moyennes de 'humeur' pour les ann√©es 2008 √† 2017 dans le dictionnaire\n",
        "    humeur_vectors_2008_2017[id_salarie] = humeur_vector_2008_2017\n",
        "\n",
        "    # Cr√©er un vecteur pour stocker les moyennes de 'humeur' pour les ann√©es 2018 √† 2023\n",
        "    humeur_vector_2018_2023 = np.zeros(len(all_dates_2018_2023), dtype=float)\n",
        "\n",
        "    # It√©ration sur chaque p√©riode de deux semaines pour les ann√©es 2018 √† 2023\n",
        "    for i, start_date in enumerate(all_dates_2018_2023['Start_Date']):\n",
        "        end_date = start_date + pd.DateOffset(weeks=2)\n",
        "        df_period = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] < end_date)]\n",
        "        if not df_period.empty:\n",
        "            humeur_vector_2018_2023[i] = pd.to_numeric(df_period['Humeur'], errors='coerce').mean()\n",
        "\n",
        "    # Stocker le vecteur de moyennes de 'humeur' pour les ann√©es 2018 √† 2023 dans le dictionnaire\n",
        "    humeur_vectors_2018_2023[id_salarie] = humeur_vector_2018_2023\n",
        "\n",
        "# # Afficher les vecteurs de moyennes de 'humeur' pour chaque salari√© pour les ann√©es 2008 √† 2017\n",
        "# print(\"Vecteurs de moyennes de 'humeur' pour les ann√©es 2008 √† 2017 :\")\n",
        "# for id_salarie, vector in humeur_vectors_2008_2017.items():\n",
        "#     print(f\"Identifiant salari√©: {id_salarie}, Vecteur de moyennes de 'humeur': {vector}\")\n",
        "\n",
        "# # Afficher les vecteurs de moyennes de 'humeur' pour chaque salari√© pour les ann√©es 2018 √† 2023\n",
        "# print(\"\\nVecteurs de moyennes de 'humeur' pour les ann√©es 2018 √† 2023 :\")\n",
        "# for id_salarie, vector in humeur_vectors_2018_2023.items():\n",
        "#     print(f\"Identifiant salari√©: {id_salarie}, Vecteur de moyennes de 'humeur': {vector}\")\n"
      ],
      "metadata": {
        "id": "GDPYVtByho5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "humeur_vectors_2018_2023[1530]"
      ],
      "metadata": {
        "id": "2Ti5PgetCtNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_meteo = [id_salarie for id_salarie in df_cor2_donnee['Identifiant salari√©'].unique() if id_salarie not in df_cor2_meteo['Identifiant salari√©'].unique()]\n",
        "\n",
        "no_accident = [id_salarie for id_salarie in df_cor2_donnee['Identifiant salari√©'].unique() if id_salarie not in df_cor2_c['Identifiant salari√©'].unique()]\n",
        "\n",
        "print('Pas de donn√©e humeur :', no_meteo)\n",
        "print('Pas de donn√©e accident :', no_accident)"
      ],
      "metadata": {
        "id": "57gB81Wx62i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It√©ration sur chaque identifiant de salari√©\n",
        "for id_salarie in df_cor2_meteo['Identifiant salari√©'].unique():\n",
        "    # R√©cup√©rer le vecteur de l'humeur correspondant √† l'identifiant du salari√©\n",
        "    humeur_vector = humeur_vectors_2018_2023.get(id_salarie)\n",
        "    accident_vector = accident_vectors_2018_2023.get(id_salarie)\n",
        "    df_cor.loc[df_cor['Identifiant salari√©'] == id_salarie, 'vecteur_humeur'] = [humeur_vector]\n",
        "    df_cor.loc[df_cor['Identifiant salari√©'] == id_salarie, 'vecteur_accident'] = [accident_vector]\n",
        "##testeter avec humeur_vectors_2018_2023['id_salarie']\n",
        "\n",
        "#     if humeur_vector is not None:\n",
        "#         # Convertir le vecteur en une cha√Æne JSON pour stockage\n",
        "#         humeur_vector_json = json.dumps(humeur_vector.tolist())\n",
        "#         accident_vector_json = json.dumps(accident_vector.tolist())\n",
        "#         # Mettre √† jour la ligne correspondante dans df_cor avec les vecteurs\n",
        "#         df_cor.loc[df_cor['Identifiant salari√©'] == id_salarie, 'vecteur_humeur'] = humeur_vector_json\n",
        "#         df_cor.loc[df_cor['Identifiant salari√©'] == id_salarie, 'vecteur_accident'] = accident_vector_json\n",
        "\n",
        "\n",
        "# df_cor\n",
        "df_cor"
      ],
      "metadata": {
        "id": "EnHwyto918oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üü¶Clustering"
      ],
      "metadata": {
        "id": "mr08FiA3vqgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clustering sans times series"
      ],
      "metadata": {
        "id": "CYSLaU1e4SJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ACP sur tout"
      ],
      "metadata": {
        "id": "8CrzdWHgwnFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lectionner les caract√©ristiques pertinentes\n",
        "features = df_cor[['Frequence accident', 'Frequence message recu', 'Frequence message envoy√©', 'Frequence demande', 'Frequence absence', 'Anciennet√©', 'Type de contrat',]]\n",
        "\n",
        "# Normaliser les donn√©es\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Appliquer l'analyse en composantes principales\n",
        "pca = PCA(n_components=2)\n",
        "features_pca = pca.fit_transform(features_scaled)\n",
        "\n",
        "# Appliquer l'algorithme K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(features_pca)\n",
        "\n",
        "# Affichage graphique des clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Tracer les points de donn√©es pour chaque cluster\n",
        "for cluster_label in range(3):\n",
        "    plt.scatter(features_pca[kmeans.labels_ == cluster_label][:, 0],\n",
        "                features_pca[kmeans.labels_ == cluster_label][:, 1],\n",
        "                label=f'Cluster {cluster_label + 1}')\n",
        "\n",
        "# Tracer les centres de chaque cluster\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='black', label='Cluster Center')\n",
        "\n",
        "plt.xlabel('Composante principale 1')\n",
        "plt.ylabel('Composante principale 2')\n",
        "plt.title('K-Means avec une ACP')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fxwie2ZODlx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trouver l'indice du centro√Øde le plus proche de chaque centre de cluster\n",
        "centroid_indices = []\n",
        "for centroid in kmeans.cluster_centers_:\n",
        "    distances = cdist(features_pca, [centroid])\n",
        "    centroid_index = np.argmin(distances)\n",
        "    centroid_indices.append(centroid_index)\n",
        "\n",
        "# R√©cup√©rer les exemples pertinents pour chaque cluster\n",
        "relevant_examples = df_cor.iloc[centroid_indices]\n",
        "\n",
        "# Afficher les exemples pertinents pour chaque cluster\n",
        "print(relevant_examples)"
      ],
      "metadata": {
        "id": "wkMFQQXV5ho1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####k-means abs accidentologie et ?"
      ],
      "metadata": {
        "id": "ixjLQ8skw-3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lectionner les caract√©ristiques pertinentes\n",
        "features = df_cor[['Frequence accident', 'Frequence message recu', 'Frequence message envoy√©', 'Frequence demande', 'Frequence absence', 'Anciennet√©', 'Type de contrat', ]]\n",
        "\n",
        "# Normaliser les donn√©es\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Choisir les axes x et y (par exemple, 'Frequence accident' et 'Anciennet√©')\n",
        "x_axis = 'Frequence accident'\n",
        "y_axis = 'Anciennet√©'\n",
        "\n",
        "# Utiliser seulement les colonnes s√©lectionn√©es pour l'algorithme K-Means\n",
        "selected_features = features[[x_axis, y_axis]]\n",
        "\n",
        "# Appliquer l'algorithme K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(selected_features)\n",
        "\n",
        "# Affichage graphique des clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Tracer les points de donn√©es pour chaque cluster\n",
        "for cluster_label in range(3):\n",
        "    plt.scatter(selected_features[kmeans.labels_ == cluster_label][x_axis],\n",
        "                selected_features[kmeans.labels_ == cluster_label][y_axis],\n",
        "                label=f'Cluster {cluster_label + 1}')\n",
        "\n",
        "# Tracer les centres de chaque cluster\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='black', label='Cluster Center')\n",
        "\n",
        "plt.xlabel(x_axis)\n",
        "plt.ylabel(y_axis)\n",
        "plt.title('K-Means sans ACP')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2a-2Xv8Qythe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On essaie d'appliquer dbscan plutot que k-means pour voir la diff√©rence car le r√©sultat est peu convaincant et ressemble √† une forme faisant penser qu'il faudrait utiliser dbscan"
      ],
      "metadata": {
        "id": "Tk7ilOrBz8kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lectionner les caract√©ristiques pertinentes\n",
        "features = df_cor[['Frequence accident', 'Frequence message recu', 'Frequence message envoy√©', 'Frequence demande', 'Frequence absence', 'Anciennet√©', 'Type de contrat']]\n",
        "\n",
        "# Normaliser les donn√©es\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Choisir les axes x et y (par exemple, 'Frequence accident' et 'Anciennet√©')\n",
        "x_axis = 'Frequence accident'\n",
        "y_axis = 'Anciennet√©'\n",
        "\n",
        "# Utiliser seulement les colonnes s√©lectionn√©es pour l'algorithme DBSCAN\n",
        "selected_features = features[[x_axis, y_axis]]\n",
        "\n",
        "# Appliquer l'algorithme DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Vous pouvez ajuster les param√®tres eps et min_samples\n",
        "dbscan_labels = dbscan.fit_predict(selected_features)\n",
        "\n",
        "# Affichage graphique des clusters\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Tracer les points de donn√©es pour chaque cluster (y compris le bruit, label√© -1 par DBSCAN)\n",
        "for cluster_label in set(dbscan_labels):\n",
        "    if cluster_label == -1:\n",
        "        plt.scatter(selected_features[dbscan_labels == cluster_label][x_axis],\n",
        "                    selected_features[dbscan_labels == cluster_label][y_axis],\n",
        "                    label=f'Noise')\n",
        "    else:\n",
        "        plt.scatter(selected_features[dbscan_labels == cluster_label][x_axis],\n",
        "                    selected_features[dbscan_labels == cluster_label][y_axis],\n",
        "                    label=f'Cluster {cluster_label + 1}')\n",
        "\n",
        "plt.xlabel(x_axis)\n",
        "plt.ylabel(y_axis)\n",
        "plt.title('DBSCAN sans ACP')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HFDPykA60DPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ACP sur tout - accident + 3d avec accident\n"
      ],
      "metadata": {
        "id": "WvsDzFNZwenO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# S√©lectionner les caract√©ristiques pertinentes\n",
        "features = df_cor[['Frequence accident', 'Frequence message recu', 'Frequence message envoy√©', 'Frequence demande', 'Frequence absence', 'Anciennet√©', 'Type de contrat']]\n",
        "\n",
        "# Normaliser les donn√©es\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Appliquer l'analyse en composantes principales\n",
        "pca = PCA(n_components=2)\n",
        "features_pca = pca.fit_transform(features_scaled)\n",
        "\n",
        "# Ajouter la fr√©quence d'accident en tant que troisi√®me dimension\n",
        "features_3d = pd.concat([pd.DataFrame(features_pca, columns=['PC1', 'PC2']), features['Frequence accident']], axis=1)\n",
        "\n",
        "# Appliquer l'algorithme K-Means\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(features_3d)\n",
        "\n",
        "# Affichage graphique en trois dimensions avec K-Means\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Tracer les points de donn√©es pour chaque cluster\n",
        "for cluster_label in range(4):\n",
        "    ax.scatter(features_3d.loc[kmeans_labels == cluster_label, 'PC1'],\n",
        "               features_3d.loc[kmeans_labels == cluster_label, 'PC2'],\n",
        "               features_3d.loc[kmeans_labels == cluster_label, 'Frequence accident'],\n",
        "               label=f'Cluster {cluster_label + 1}')\n",
        "\n",
        "# Tracer les centres de chaque cluster\n",
        "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2],\n",
        "           marker='x', color='black', label='Cluster Center')\n",
        "\n",
        "ax.set_xlabel('Composante principale 1')\n",
        "ax.set_ylabel('Composante principale 2')\n",
        "ax.set_zlabel('Frequence accident')\n",
        "ax.set_title('K-Means avec ACP en 3D')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Eb5XQ0Jr2oBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trouver l'indice du centro√Øde le plus proche de chaque centre de cluster\n",
        "centroid_indices = []\n",
        "for centroid in kmeans.cluster_centers_:\n",
        "    distances = cdist(features_3d[['PC1', 'PC2', 'Frequence accident']], [centroid])\n",
        "    centroid_index = np.argmin(distances)\n",
        "    centroid_indices.append(centroid_index)\n",
        "\n",
        "# R√©cup√©rer les exemples pertinents pour chaque cluster\n",
        "relevant_examples = df_cor.iloc[centroid_indices]\n",
        "\n",
        "# Afficher les exemples pertinents pour chaque cluster\n",
        "print(relevant_examples)"
      ],
      "metadata": {
        "id": "WJvxyxWiA2kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Clustering avec times series"
      ],
      "metadata": {
        "id": "IweLYoX04aCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîéPremiere analyse √† l'aide de sweetviz"
      ],
      "metadata": {
        "id": "EY7S1T3F1hOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sweetviz"
      ],
      "metadata": {
        "id": "YJdjQcn29ULV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sweetviz as sv"
      ],
      "metadata": {
        "id": "8poBadZi9Vz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_bivariate_fo = sv.analyze(source=df_cor,\n",
        "                                 target_feat='Frequence accident', # add focus on our target value\n",
        "                                 feat_cfg=None,\n",
        "                                 pairwise_analysis='on') # let's see the relation btw our variables\n",
        "report_bivariate_fo.show_html(filepath=\"reports.html\")\n",
        "report_bivariate_fo.show_notebook(w=2000, h=1000)"
      ],
      "metadata": {
        "id": "HL06WUQN0xe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üîéCorr√©lation"
      ],
      "metadata": {
        "id": "Kd07eISsY1ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Corr√©lation avec identifiant salari√© unique"
      ],
      "metadata": {
        "id": "yqLdQP1rKGXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_correlation_with_feature_v2(df_cor,'Frequence accident')"
      ],
      "metadata": {
        "id": "6ID172ArIZ09"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KiI1rArukGDd",
        "NQDcbwVjhmnY",
        "incFvmiIbBoN",
        "oNz7mfcJCu5j",
        "gdJTfIwDodU4",
        "1UxPC0xhnslV",
        "bdQ48VWInswp",
        "-D1mGyPBns2B",
        "Ooy55atZnySc"
      ],
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}